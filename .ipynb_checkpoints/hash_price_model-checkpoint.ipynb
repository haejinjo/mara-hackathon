{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34dfd5c5-b82c-460b-942d-b48eb2413d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline         \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error       \n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec68d37b-646b-44ad-8d7c-4040dff8b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the three already-saved CSVs\n",
    "hp = (pd.read_csv(\"hashprice.csv\", parse_dates=[\"timestamp\"])\n",
    "        # convert UTC seconds → America/Los_Angeles midnight\n",
    "        .assign(date=lambda df:\n",
    "                pd.to_datetime(df[\"timestamp\"], unit=\"s\", utc=True)\n",
    "                  .dt.tz_convert(\"America/Los_Angeles\")\n",
    "                  .dt.normalize())\n",
    "        .drop(columns=[\"timestamp\"])\n",
    "        .rename(columns={\"usd_hashprice\": \"hash_usd\",\n",
    "                         \"btc_hashprice\": \"hash_btc\"})\n",
    "        .set_index(\"date\")\n",
    "        .sort_index())\n",
    "\n",
    "# 2⎯ pick the modelling target (here USD hash-price)\n",
    "hp = hp.rename(columns={\"usd_hashprice\": \"hash_usd\",      # target\n",
    "                        \"btc_hashprice\": \"hash_btc\"}) \n",
    "hp.index = hp.index.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7368b6ad-9d56-4a90-8a8c-ee8a38752fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = (pd.read_csv(\"fng.csv\", parse_dates=[\"date\"], dayfirst=True)\n",
    "          .rename(columns={\n",
    "              \"fng_vale\": \"sentiment\",          # numeric 0–100\n",
    "              \"fng_classification\": \"sentiment_class\"\n",
    "          })\n",
    "          .set_index(\"date\")\n",
    "          .sort_index())\n",
    "sent.index = sent.index.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90fef5da-8f14-4a6e-8051-5d9438a4a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1⎯ read, ignoring the metadata row\n",
    "wx_raw = pd.read_csv(\n",
    "    \"weather.csv\",\n",
    "    skiprows=2,                 # drops the lat/long row\n",
    "    parse_dates=[\"time\"]        # converts the date string to Timestamp\n",
    ")\n",
    "\n",
    "# 2⎯ rename the columns to concise identifiers\n",
    "wx = (wx_raw\n",
    "       .assign(date=lambda df: pd.to_datetime(df[\"time\"])\n",
    "                               .dt.tz_localize(\"UTC\")\n",
    "                               .dt.tz_convert(\"America/Los_Angeles\")\n",
    "                               .dt.normalize()\n",
    "                               .dt.tz_localize(None))\n",
    "       .set_index(\"date\")\n",
    "       .rename(columns={\"precipitation_sum (mm)\": \"precip\",\n",
    "                        \"temperature_2m_mean (°C)\": \"temp_mean\"}))\n",
    "wx = wx.iloc[:, :2]\n",
    "wx.index = wx.index.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b57e4ef-c37e-4347-9934-50c408dbcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (hp, sent, wx):\n",
    "    df.index = pd.to_datetime(df.index)   # safe even if already datetime\n",
    "    df.sort_index(inplace=True)           # chronological order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a014e78-99c6-4965-8d99-5c43697ec15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (hp, sent, wx):\n",
    "    df.index = pd.to_datetime(df.index)          # guarantee `DatetimeIndex`\n",
    "    df.sort_index(inplace=True)                 # chronological order\n",
    "combined = pd.concat([hp, sent, wx], axis=1, join='inner')   # or 'outer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "abf271f3-9976-4e43-bbe1-54c18cf067af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp  = hp.add_prefix(\"hp_\")     # hp_time, hp_price, …\n",
    "sent = sent.add_prefix(\"sent_\")\n",
    "wx   = wx.add_prefix(\"wx_\")\n",
    "combined = pd.concat([hp, sent, wx], axis=1, join=\"outer\")  # keeps union of dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71cd7753-5e3e-44b8-a651-e5ebc6be6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            hp_hash_usd  hp_hash_btc  sent_fng_value sent_sentiment_class  \\\n",
      "date                                                                        \n",
      "2020-06-20        76.73     0.008208             NaN                  NaN   \n",
      "2020-06-21        77.87     0.008214             NaN                  NaN   \n",
      "2020-06-22        80.25     0.008326             NaN                  NaN   \n",
      "2020-06-23        78.82     0.008348             NaN                  NaN   \n",
      "2020-06-24        76.89     0.008329             NaN                  NaN   \n",
      "...                 ...          ...             ...                  ...   \n",
      "2025-06-17        52.97     0.000506            68.0                Greed   \n",
      "2025-06-18        52.73     0.000504            52.0              Neutral   \n",
      "2025-06-19        52.62     0.000503            57.0                Greed   \n",
      "2025-06-20        51.91     0.000503            54.0              Neutral   \n",
      "2025-06-21          NaN          NaN            49.0              Neutral   \n",
      "\n",
      "              wx_time  wx_temp_mean  \n",
      "date                                 \n",
      "2020-06-20        NaT           NaN  \n",
      "2020-06-21        NaT           NaN  \n",
      "2020-06-22        NaT           NaN  \n",
      "2020-06-23        NaT           NaN  \n",
      "2020-06-24        NaT           NaN  \n",
      "...               ...           ...  \n",
      "2025-06-17 2025-06-18          16.9  \n",
      "2025-06-18 2025-06-19           NaN  \n",
      "2025-06-19 2025-06-20           NaN  \n",
      "2025-06-20        NaT           NaN  \n",
      "2025-06-21        NaT           NaN  \n",
      "\n",
      "[1828 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4971157-2b2f-4257-989b-7af385129d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined.copy()                     # keep original untouched\n",
    "\n",
    "# 1.1  drop the redundant wx_time column\n",
    "df = df.drop(columns=\"wx_time\")\n",
    "\n",
    "# 1.2  convert the sentiment class to one-hot dummies\n",
    "df = pd.get_dummies(df, columns=[\"sent_sentiment_class\"], prefix=\"sent_cls\", dummy_na=False)\n",
    "\n",
    "# 1.3  handle missing numeric values – simplest: forward-fill then back-fill\n",
    "df = df.ffill().bfill()\n",
    "\n",
    "# 1.4  scale all numeric columns except the target\n",
    "num_cols = df.columns.drop(\"hp_hash_usd\")\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6d831e64-2224-40a7-8a05-2b6a5a498add",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back  = 30      # L = 30 past days\n",
    "horizon    = 1       # predict next day\n",
    "target_col = df.columns.get_loc(\"hp_hash_usd\")\n",
    "\n",
    "X, y = [], []\n",
    "values = df.values            # 2-D NumPy array (rows × columns)\n",
    "for i in range(look_back, len(values) - horizon + 1):\n",
    "    X.append(values[i-look_back:i])               # shape (L, d)\n",
    "    y.append(values[i + horizon - 1, target_col]) # scalar\n",
    "X = np.stack(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "acebb310-7123-4e7f-8af6-1cce39610fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n  = len(X)\n",
    "n_train = int(0.6 * n)\n",
    "n_val   = int(0.2 * n)\n",
    "\n",
    "X_train, y_train = X[:n_train],           y[:n_train]\n",
    "X_val,   y_val   = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "X_test,  y_test  = X[n_train+n_val:],     y[n_train+n_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06f8656d-ef53-45d2-a277-a8a67f9d2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HashPriceGRU(nn.Module):\n",
    "    def __init__(self, d_in, hidden=32):\n",
    "        super().__init__()\n",
    "        self.gru  = nn.GRU(d_in, hidden, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)           # h shape: (1, batch, hidden)\n",
    "        return self.head(h.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "16905177-4d47-4890-8f1f-1bae11ce772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    "), batch_size=32, shuffle=False)   # keep order within each batch\n",
    "\n",
    "val_loader = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    "), batch_size=32, shuffle=False)\n",
    "\n",
    "model = HashPriceGRU(d_in=X.shape[2]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience, epochs_no_improve = 10, 0\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb).squeeze(1)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(\n",
    "            criterion(model(xv.to(device)).squeeze(1), yv.to(device)).item()\n",
    "            for xv, yv in val_loader\n",
    "        ) / len(val_loader)\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), \"best_gru.pt\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a368c196-e0c1-4038-a37b-b68623b2d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(days: int = 730, start: str = \"2023-01-01\") -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    dates = pd.date_range(start, periods=days, freq=\"D\")\n",
    "\n",
    "    base_price = 100.0\n",
    "    returns = rng.normal(loc=0.0002, scale=0.01, size=days)\n",
    "    price_series = base_price * np.exp(np.cumsum(returns))\n",
    "\n",
    "    hash_usd = price_series / 2.0 + rng.normal(0, 1.0, size=days)\n",
    "    hash_btc = hash_usd / 40000.0\n",
    "\n",
    "    temp_mean = 12 + 8*np.sin(2*np.pi*np.arange(days)/365) + rng.normal(0, 1.5, size=days)\n",
    "\n",
    "    sentiment_val = rng.uniform(0, 100, size=days)\n",
    "\n",
    "    def label(v):\n",
    "        if v < 25:\n",
    "            return \"Extreme_Fear\"\n",
    "        elif v < 50:\n",
    "            return \"Fear\"\n",
    "        elif v < 75:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Greed\"\n",
    "    sentiment_cls = np.vectorize(label)(sentiment_val)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"hp_hash_usd\": hash_usd,\n",
    "        \"hp_hash_btc\": hash_btc,\n",
    "        \"sent_fng_value\": sentiment_val,\n",
    "        \"sent_sentiment_class\": sentiment_cls,\n",
    "        \"wx_temp_mean\": temp_mean,\n",
    "    }, index=dates)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e8fe1273-0e73-4788-b23a-f33c8da5e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (MLP): 10.4679\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.625755</td>\n",
       "      <td>48.568123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.062917</td>\n",
       "      <td>48.900768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.820216</td>\n",
       "      <td>50.358445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.206878</td>\n",
       "      <td>48.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.573406</td>\n",
       "      <td>49.172915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred     actual\n",
       "0  30.625755  48.568123\n",
       "1  34.062917  48.900768\n",
       "2  28.820216  50.358445\n",
       "3  34.206878  48.002388\n",
       "4  36.573406  49.172915"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_supervised(df: pd.DataFrame, look_back: int = 30, horizon: int = 1):\n",
    "    \"\"\"\n",
    "    Convert a daily DataFrame into supervised learning matrices (features, target)\n",
    "    using a sliding window. For sklearn we flatten each window into 1-D.\n",
    "    \"\"\"\n",
    "    vals = df.values\n",
    "    target_idx = df.columns.get_loc(\"hp_hash_usd\")\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(vals) - horizon + 1):\n",
    "        window = vals[i-look_back:i].flatten()      # shape (look_back * d,)\n",
    "        X.append(window)\n",
    "        y.append(vals[i + horizon - 1, target_idx])\n",
    "    return np.stack(X), np.array(y)\n",
    "\n",
    "def train_run(days: int = 730):\n",
    "    # 1. generate data\n",
    "    df = generate_random_data(days)\n",
    "    # 2. preprocess: one-hot encode categorical sentiment\n",
    "    df_proc = pd.get_dummies(df, columns=[\"sent_sentiment_class\"], prefix=\"sent_cls\")\n",
    "    df_proc = df_proc.ffill().bfill()\n",
    "\n",
    "    # 3. make supervised dataset\n",
    "    L = 30\n",
    "    X, y = make_supervised(df_proc, look_back=L, horizon=1)\n",
    "\n",
    "    # 4. chronological split\n",
    "    n = len(X)\n",
    "    n_train = int(0.6 * n)\n",
    "    n_val   = int(0.2 * n)\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val,   y_val   = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "    X_test,  y_test  = X[n_train+n_val:], y[n_train+n_val:]\n",
    "\n",
    "    # 5. model: StandardScaler + MLPRegressor\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", MLPRegressor(hidden_layer_sizes=(64, 32),\n",
    "                             activation=\"relu\",\n",
    "                             solver=\"adam\",\n",
    "                             learning_rate_init=1e-3,\n",
    "                             max_iter=500,\n",
    "                             random_state=42))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 6. evaluation\n",
    "    pred_test = model.predict(X_test)\n",
    "    mse  = mean_squared_error(y_test, pred_test)  # returns MSE\n",
    "    rmse = np.sqrt(mse)     \n",
    "\n",
    "    # 7. show first 5 predictions vs actual\n",
    "    res = pd.DataFrame({\"pred\": pred_test[:5], \"actual\": y_test[:5]})\n",
    "    print(f\"Test RMSE (MLP): {rmse:.4f}\")\n",
    "    return res\n",
    "\n",
    "results_head = train_run()\n",
    "results_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5fc4485d-b152-4d27-9a9e-fbc7a61e5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (MLP): 10.4679\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.625755</td>\n",
       "      <td>48.568123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.062917</td>\n",
       "      <td>48.900768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.820216</td>\n",
       "      <td>50.358445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.206878</td>\n",
       "      <td>48.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.573406</td>\n",
       "      <td>49.172915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred     actual\n",
       "0  30.625755  48.568123\n",
       "1  34.062917  48.900768\n",
       "2  28.820216  50.358445\n",
       "3  34.206878  48.002388\n",
       "4  36.573406  49.172915"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "726f540f-2802-4c9f-a24b-2f81c853c2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[185]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# === 7. test predictions ===\u001b[39;00m\n\u001b[32m    106\u001b[39m pred_1d = predict_hash_usd_future(model, history_dicts, nday=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m pred_3d = \u001b[43mpredict_hash_usd_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnday\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction 1 day ahead:\u001b[39m\u001b[33m\"\u001b[39m, pred_1d)\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction 3 days ahead:\u001b[39m\u001b[33m\"\u001b[39m, pred_3d)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[185]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mpredict_hash_usd_future\u001b[39m\u001b[34m(model, history, nday)\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhistory must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLOOK_BACK\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days long\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m window = history.copy()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnday\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     77\u001b[39m     X_window = np.vstack([_prepare_single_row(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m window]).flatten()[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[32m     78\u001b[39m     y_pred = model.predict(X_window)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from typing import List, Dict\n",
    "\n",
    "# === 1. constants and helper functions ===\n",
    "LOOK_BACK = 30\n",
    "NUMERIC_VARS = [\"hp_hash_usd\", \"hp_hash_btc\", \"sent_fng_value\", \"wx_temp_mean\"]\n",
    "CATEG_LABELS = [\"Extreme_Fear\", \"Fear\", \"Neutral\", \"Greed\"]\n",
    "\n",
    "def _one_hot(label: str) -> np.ndarray:\n",
    "    vec = np.zeros(len(CATEG_LABELS), dtype=float)\n",
    "    try:\n",
    "        vec[CATEG_LABELS.index(label)] = 1.0\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return vec\n",
    "\n",
    "def _prepare_single_row(row_dict: Dict) -> np.ndarray:\n",
    "    numeric_part = np.array([row_dict[k] for k in NUMERIC_VARS], dtype=float)\n",
    "    one_hot_part = _one_hot(row_dict[\"sent_sentiment_class\"])\n",
    "    return np.concatenate([numeric_part, one_hot_part])\n",
    "\n",
    "# === 2. random synthetic data generator ===\n",
    "def generate_random_data(days: int = 365, start: str = \"2024-01-01\") -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed=0)\n",
    "    dates = pd.date_range(start, periods=days, freq=\"D\")\n",
    "    \n",
    "    base_price = 100.0\n",
    "    returns = rng.normal(loc=0.0001, scale=0.01, size=days)\n",
    "    price_series = base_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    hash_usd = price_series / 2 + rng.normal(0, 1, size=days)\n",
    "    hash_btc = hash_usd / 40000.0\n",
    "    sentiment_val = rng.uniform(0, 100, size=days)\n",
    "    temp_mean = 15 + 5*np.sin(2*np.pi*np.arange(days)/365) + rng.normal(0, 1.5, size=days)\n",
    "    \n",
    "    def label(v):\n",
    "        if v < 25:\n",
    "            return \"Extreme_Fear\"\n",
    "        elif v < 50:\n",
    "            return \"Fear\"\n",
    "        elif v < 75:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Greed\"\n",
    "    sentiment_cls = np.vectorize(label)(sentiment_val)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"hp_hash_usd\": hash_usd,\n",
    "        \"hp_hash_btc\": hash_btc,\n",
    "        \"sent_fng_value\": sentiment_val,\n",
    "        \"sent_sentiment_class\": sentiment_cls,\n",
    "        \"wx_temp_mean\": temp_mean,\n",
    "    }, index=dates)\n",
    "\n",
    "# === 3. make supervised matrix ===\n",
    "def make_supervised(df: pd.DataFrame, look_back: int = LOOK_BACK):\n",
    "    df_enc = pd.get_dummies(df, columns=[\"sent_sentiment_class\"], prefix=\"sent_cls\")\n",
    "    df_enc = df_enc.ffill().bfill()\n",
    "    values = df_enc.values\n",
    "    X, y = [], []\n",
    "    target_idx = df_enc.columns.get_loc(\"hp_hash_usd\")\n",
    "    for i in range(look_back, len(values)):\n",
    "        X.append(values[i-look_back:i].flatten())\n",
    "        y.append(values[i, target_idx])\n",
    "    return np.stack(X), np.array(y)\n",
    "\n",
    "# === 4. prediction function ===\n",
    "def predict_hash_usd_future(model: Pipeline, history: List[Dict], nday: int = 1) -> float:\n",
    "    if len(history) != LOOK_BACK:\n",
    "        raise ValueError(f\"history must be {LOOK_BACK} days long\")\n",
    "    window = history.copy()\n",
    "    for _ in range(nday):\n",
    "        X_window = np.vstack([_prepare_single_row(d) for d in window]).flatten()[None, :]\n",
    "        y_pred = model.predict(X_window)[0]\n",
    "        last = window[-1]\n",
    "        synthetic_next = {\n",
    "            \"hp_hash_usd\": y_pred,\n",
    "            \"hp_hash_btc\": y_pred / 40000.0,\n",
    "            \"sent_fng_value\": last[\"sent_fng_value\"],\n",
    "            \"wx_temp_mean\": last[\"wx_temp_mean\"],\n",
    "            \"sent_sentiment_class\": last[\"sent_sentiment_class\"]\n",
    "        }\n",
    "        window.append(synthetic_next)\n",
    "        window.pop(0)  # keep length constant\n",
    "    return y_pred\n",
    "\n",
    "# === 5. train a simple model on synthetic data ===\n",
    "df_rand = generate_random_data(400)\n",
    "X_train, y_train = make_supervised(df_rand)\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linreg\", LinearRegression())\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 6. prepare a random 30-day history window ===\n",
    "history_frame = generate_random_data(LOOK_BACK, start=\"2025-01-01\")\n",
    "history_dicts = history_frame.to_dict(orient=\"records\")\n",
    "\n",
    "# === 7. test predictions ===\n",
    "pred_1d = predict_hash_usd_future(model, history_dicts, nday=1)\n",
    "pred_3d = predict_hash_usd_future(model, history_dicts, nday=2)\n",
    "\n",
    "print(\"Prediction 1 day ahead:\", pred_1d)\n",
    "print(\"Prediction 3 days ahead:\", pred_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93568da7-c176-457a-8c5d-9556619ffff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(mara-env)",
   "language": "python",
   "name": "mara-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
